{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:27.257194Z",
     "start_time": "2024-04-13T20:14:26.113820Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:27.388293Z",
     "start_time": "2024-04-13T20:14:27.258195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   index  object_id                                               name  \\\n0   1363   19943072                                          Пластинка   \n1   3147    9221257  Каталог. Первая Бельгïйская Художественная Выс...   \n2   4864   43808403               Письмо к Новикову Ивану Алексеевичу.   \n3   4527   19941202                                           Бинокль.   \n4   4771   21625781                         Радиоприемник «Домбай-303»   \n\n                                         description         group  \\\n0  Всероссийское театральное общество ВТО, фабрик...        Прочие   \n1  В бумажной обложке с полихромной печатью титул...  Редкие книги   \n2  С просьбой назначить представителя от Госиздат...     Документы   \n3  Бинокль. Производитель – CP GOERZ (оттиск штам...       Техника   \n4  Черного цвета, серая металлическая вставка. в ...       Техника   \n\n       img_name                             img_path  \\\n0  20650863.jpg  dataset/train/19943072/20650863.jpg   \n1  31817178.jpg   dataset/train/9221257/31817178.jpg   \n2  53905453.jpg  dataset/train/43808403/53905453.jpg   \n3  20648209.jpg  dataset/train/19941202/20648209.jpg   \n4  22984135.jpg  dataset/train/21625781/22984135.jpg   \n\n                                          translated  \n0  All -Russian Theater Society of the WTO, a fac...  \n1  In the paper cover with the polychrome printin...  \n2  With a request to appoint a representative fro...  \n3  Binoculars.Manufacturer - CP Goerz (print of t...  \n4  Black, gray metal insert.In the lower part of ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>object_id</th>\n      <th>name</th>\n      <th>description</th>\n      <th>group</th>\n      <th>img_name</th>\n      <th>img_path</th>\n      <th>translated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1363</td>\n      <td>19943072</td>\n      <td>Пластинка</td>\n      <td>Всероссийское театральное общество ВТО, фабрик...</td>\n      <td>Прочие</td>\n      <td>20650863.jpg</td>\n      <td>dataset/train/19943072/20650863.jpg</td>\n      <td>All -Russian Theater Society of the WTO, a fac...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3147</td>\n      <td>9221257</td>\n      <td>Каталог. Первая Бельгïйская Художественная Выс...</td>\n      <td>В бумажной обложке с полихромной печатью титул...</td>\n      <td>Редкие книги</td>\n      <td>31817178.jpg</td>\n      <td>dataset/train/9221257/31817178.jpg</td>\n      <td>In the paper cover with the polychrome printin...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4864</td>\n      <td>43808403</td>\n      <td>Письмо к Новикову Ивану Алексеевичу.</td>\n      <td>С просьбой назначить представителя от Госиздат...</td>\n      <td>Документы</td>\n      <td>53905453.jpg</td>\n      <td>dataset/train/43808403/53905453.jpg</td>\n      <td>With a request to appoint a representative fro...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4527</td>\n      <td>19941202</td>\n      <td>Бинокль.</td>\n      <td>Бинокль. Производитель – CP GOERZ (оттиск штам...</td>\n      <td>Техника</td>\n      <td>20648209.jpg</td>\n      <td>dataset/train/19941202/20648209.jpg</td>\n      <td>Binoculars.Manufacturer - CP Goerz (print of t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4771</td>\n      <td>21625781</td>\n      <td>Радиоприемник «Домбай-303»</td>\n      <td>Черного цвета, серая металлическая вставка. в ...</td>\n      <td>Техника</td>\n      <td>22984135.jpg</td>\n      <td>dataset/train/21625781/22984135.jpg</td>\n      <td>Black, gray metal insert.In the lower part of ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('translated_full.csv', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[\"img_path\"] = df[\"img_path\"].apply(lambda x: x.replace(\"dataset\", \"E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\\data\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:27.396312Z",
     "start_time": "2024-04-13T20:14:27.389291Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:27.570970Z",
     "start_time": "2024-04-13T20:14:27.397314Z"
    }
   },
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "def translate_text(text, dest_lang='ru', src='en'):\n",
    "    translation = translator.translate(text, dest=dest_lang, src=src)\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:27.575004Z",
     "start_time": "2024-04-13T20:14:27.571970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "10280"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:28.861587Z",
     "start_time": "2024-04-13T20:14:27.576003Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df['group'], test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:29.410775Z",
     "start_time": "2024-04-13T20:14:28.862611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\\data/train/27501326/31231304.jpg!****!A round shape with a square hole in the center, on the front side - four hieroglyphs located crossly on the back - two side hieroglyphs.\n"
     ]
    }
   ],
   "source": [
    "my_dataset = []\n",
    "for i in range(len(X_train)):\n",
    "    my_dataset.append(f'{X_train.iloc[i][\"img_path\"]}!****!{X_train.iloc[i][\"translated\"]}')\n",
    "print(my_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:33.801154Z",
     "start_time": "2024-04-13T20:14:29.411790Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=np.array(Image.open(item.split(\"!****!\")[0])), text=\"a photography of\", padding=\"max_length\", return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            item.split(\"!****!\")[1], max_length= 512, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:40.692338Z",
     "start_time": "2024-04-13T20:14:33.802155Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:40.697792Z",
     "start_time": "2024-04-13T20:14:40.693337Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(my_dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:14:41.590722Z",
     "start_time": "2024-04-13T20:14:40.698791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\\data/train/27501326/31231304.jpg!****!A round shape with a square hole in the center, on the front side - four hieroglyphs located crossly on the back - two side hieroglyphs.\n"
     ]
    }
   ],
   "source": [
    "my_dataset = []\n",
    "for i in range(len(X_train)):\n",
    "    my_dataset.append(f'{X_train.iloc[i][\"img_path\"]}!****!{X_train.iloc[i][\"translated\"]}')\n",
    "print(my_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:15:19.049617Z",
     "start_time": "2024-04-13T20:14:41.592717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AnacondaNew\\envs\\ml-course-spbu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 0. Loss: 10.34445858001709\n",
      "Idx 0. Loss: 10.34445858001709\n",
      "Idx 1. Loss: 10.260732650756836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIdx \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Loss:\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m idx \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 30\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_weights.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     31\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     33\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mE:\\AnacondaNew\\envs\\ml-course-spbu\\Lib\\site-packages\\torch\\serialization.py:628\u001B[0m, in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    626\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m--> 628\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001B[0;32m    629\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\AnacondaNew\\envs\\ml-course-spbu\\Lib\\site-packages\\torch\\serialization.py:862\u001B[0m, in \u001B[0;36m_save\u001B[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001B[39;00m\n\u001B[0;32m    861\u001B[0m num_bytes \u001B[38;5;241m=\u001B[39m storage\u001B[38;5;241m.\u001B[39mnbytes()\n\u001B[1;32m--> 862\u001B[0m zip_file\u001B[38;5;241m.\u001B[39mwrite_record(name, storage, num_bytes)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "count = 0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    #torch.save(model.state_dict(), '/content/models/model_weights.pth')\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "        labels = batch.pop(\"labels\").to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        labels=input_ids)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(f\"Idx {idx}. Loss:\", loss.item())\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Idx {idx}. Loss:\", loss.item())\n",
    "\n",
    "        if idx // 100 == 0:\n",
    "            torch.save(model.state_dict(), 'model_weights.pth')\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/10498937/7631593.jpg!****!Avers: the coat of arms of the USSR below the abbreviation \"S.S.S.r.\"On the edge of the coin - a linear rim.revers: the two -line inscription \"2 kopecks\", below the date \"1941\", on the sides of wheat ears, along the edge of the coin linear rim. Gurt is sauced.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = []\n",
    "for i in range(len(X_test)):\n",
    "    test_dataset.append(f'{X_test.iloc[i][\"img_path\"]}!****!{X_test.iloc[i][\"translated\"]}')\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for example in test_dataset:\n",
    "\n",
    "    image = example.split(\"!****!\")[0]\n",
    "    img = Image.open(image)\n",
    "    display(img)\n",
    "    \n",
    "    \n",
    "    # prepare image for the model\n",
    "    inputs = processor(images=np.array(img), text=\"a photography of\", padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids)\n",
    "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(generated_caption+\"TTTTT\"+example.split(\"!****!\")[1])\n",
    "    time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

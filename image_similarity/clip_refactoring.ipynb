{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e6f048ea3f2aed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "print(inputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38cee9bcfd5e45fd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7ed0df2cc3c00e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e517eb8e8c70e718"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "24b9a4779fb83aa0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to()\n",
    "\n",
    "# # Get the text features\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# \n",
    "# inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "# text_features = model.get_text_features(**inputs)\n",
    "# \n",
    "# print(text_features.shape) # output shape of text features\n",
    "\n",
    "# Get the image features\n",
    "model_ckpt = \"openai/clip-vit-large-patch14\"\n",
    "processor = AutoProcessor.from_pretrained(model_ckpt)\n",
    "# extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n",
    "# model = AutoModel.from_pretrained(model_ckpt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ca359bcfdcafa01",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_embeddings(image):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    return image_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca22532465b25a6f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_scores(emb_one, emb_two):\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    scores = torch.nn.functional.cosine_similarity(emb_one, emb_two)\n",
    "    # print(scores.data.numpy())\n",
    "    return scores.data.numpy().tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c066778f2908218",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "compute_scores(get_embeddings(Image.open(\"data/ex1.jpg\")), get_embeddings(Image.open(\"data/ex2.jpg\")))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aacd13ceefefe28",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8301c3c6e1aba60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# Data transformation chain.\n",
    "transformation_chain = T.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256 and then we take center crop.\n",
    "        T.Resize((256, 256)),\n",
    "        # T.CenterCrop(extractor.size[\"height\"]),\n",
    "        # T.ToTensor(),\n",
    "        # T.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6fb0a4834a08867",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "dir_pth = \"data/test\"\n",
    "\n",
    "for img_dir in os.listdir(dir_pth):\n",
    "    for img in os.listdir(os.path.join(dir_pth, img_dir)):\n",
    "        shutil.move(os.path.join(dir_pth, img_dir, img), os.path.join(dir_pth, img))\n",
    "    os.rmdir(os.path.join(dir_pth, img_dir))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e52cb802de7f254",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dir_pth = \"data/my_dataset\"\n",
    "image_file_path = []\n",
    "image = []\n",
    "labels = []\n",
    "for img_dir in os.listdir(dir_pth):\n",
    "    for img in os.listdir(os.path.join(dir_pth, img_dir)):\n",
    "        pp = os.path.join(dir_pth, img_dir, img)\n",
    "        image_file_path += [pp]\n",
    "        image += [Image.open(pp)]\n",
    "        labels += [f\"folder_{str(img_dir)}_img_{img}\"]\n",
    "    \n",
    "res = {'image_file_path': image_file_path,\n",
    "       'image': image,\n",
    "       'labels': labels}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b814ee5cd9286296",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict(res)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ce840839e4c9ea1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset.features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cee111a8a406a0bf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# \n",
    "# dataset = load_dataset(\"beans\")\n",
    "# dataset[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f38c4d2e633c0cb4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad5368d976c10c41",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "seed = 42\n",
    "batch_size = 16\n",
    "candidate_subset = dataset.shuffle(seed=seed).select(range(num_samples))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d028afab4db2db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_embeddings(model: torch.nn.Module):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        images = batch[\"image\"]\n",
    "        # `transformation_chain` is a compostion of preprocessing\n",
    "        # transformations we apply to the input images to prepare them\n",
    "        # for the model. For more details, check out the accompanying Colab Notebook.\n",
    "        images = [transformation_chain(image) for image in images]\n",
    "        # print(image_batch_transformed)\n",
    "        # new_batch = {\"pixel_values\": image_batch_transformed.to(device)}\n",
    "        with torch.no_grad():\n",
    "            embeddings = get_embeddings(images)\n",
    "        return {\"embeddings\": embeddings}\n",
    "\n",
    "    return pp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6c8dd87f33cdded",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_fn = extract_embeddings(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f16eef547c9efa3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "candidate_subset_emb = candidate_subset.map(extract_fn, batched=True, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86c407b694cd084d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_candidate_embeddings = np.array(candidate_subset_emb[\"embeddings\"])\n",
    "all_candidate_embeddings = torch.from_numpy(all_candidate_embeddings)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d69c202b6c19a3f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "candidate_ids = []\n",
    "\n",
    "for id in tqdm(range(len(candidate_subset_emb))):\n",
    "    label = candidate_subset_emb[id][\"labels\"]\n",
    "\n",
    "    # Create a unique indentifier.\n",
    "    entry = str(id) + \"_\" + str(label)\n",
    "\n",
    "    candidate_ids.append(entry)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24a69e4f9b62702a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "list(filter(lambda x: \"folder_1\" in x and \"img_4192145\" in x, candidate_ids))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a29d4f75cd3d699",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fetch_similar(image, top_k=5):\n",
    "    \"\"\"Fetches the `top_k` similar images with `image` as the query.\"\"\"\n",
    "    # Prepare the input query image for embedding computation.\n",
    "    # image_transformed = transformation_chain(image).unsqueeze(0)\n",
    "\n",
    "    # Comute the embedding.\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = get_embeddings(image)\n",
    "\n",
    "    # Compute similarity scores with all the candidate images at one go.\n",
    "    # We also create a mapping between the candidate image identifiers\n",
    "    # and their similarity scores with the query image.\n",
    "    sim_scores = compute_scores(all_candidate_embeddings, query_embeddings)\n",
    "    similarity_mapping = dict(zip(candidate_ids, sim_scores))\n",
    "\n",
    "    # Sort the mapping dictionary and return `top_k` candidates.\n",
    "    similarity_mapping_sorted = dict(\n",
    "        sorted(similarity_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "    id_entries = list(similarity_mapping_sorted.keys())[:top_k]\n",
    "\n",
    "    ids = list(map(lambda x: int(x.split(\"_\")[0]), id_entries))\n",
    "    labels = list(map(lambda x: x, id_entries))\n",
    "    return ids, labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a6220e9ae6fd0dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6b1b23ac6de8876",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_idx = 118 # np.random.choice(len(dataset))\n",
    "test_sample = candidate_subset_emb[test_idx][\"image\"]\n",
    "test_label = candidate_subset_emb[\"labels\"]\n",
    "\n",
    "k = 10\n",
    "sim_ids, sim_labels = fetch_similar(test_sample, top_k=k)\n",
    "print(f\"Query label: {test_label}\")\n",
    "print(f\"Top {k} candidate labels: {sim_labels}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "633603feffdb5198",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i, pp in enumerate(os.listdir(\"data/my_dataset\")):\n",
    "    os.rename(os.path.join(\"data/my_dataset\", pp), os.path.join(\"data/my_dataset\", str(i)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "860df2aaff700a5a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "paths = list(map(lambda x: get_path_img(\"data/my_dataset\", x), sim_labels))\n",
    "print(paths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42d5d541adf8dd63",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_path_img(dir_name, img):\n",
    "    splitted = img.split(\"_\")\n",
    "    fold = splitted[2]\n",
    "    img_name = splitted[4]\n",
    "    return os.path.join(dir_name, fold, img_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa232e311159700c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "images = [Image.open(x) for x in paths]\n",
    "widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "total_width = sum(widths)\n",
    "max_height = max(heights)\n",
    "\n",
    "new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "x_offset = 0\n",
    "for im in images:\n",
    "    new_im.paste(im, (x_offset,0))\n",
    "    x_offset += im.size[0]\n",
    "\n",
    "new_im.save('test.jpg')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47e88e447cbbb525",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba2494ac30bb392a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from transformers import AutoProcessor, CLIPModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:22:48.998067Z",
     "start_time": "2024-04-13T12:22:46.162914Z"
    }
   },
   "id": "1ca7511682d7d7d8",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "418e70af6135f3df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:22:49.017535Z",
     "start_time": "2024-04-13T12:22:48.999075Z"
    }
   },
   "id": "46ebea7bea671422",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name_1 = \"openai/clip-vit-large-patch14\"\n",
    "model_name_2 = \"Salesforce/blip-image-captioning-large\" #\n",
    "\n",
    "model_1 = CLIPModel.from_pretrained(model_name_1).to(device)\n",
    "processor_1 = AutoProcessor.from_pretrained(model_name_1)\n",
    "\n",
    "# model_2 = BlipModel.from_pretrained(model_name_2).to(device)\n",
    "# processor_2 = AutoProcessor.from_pretrained(model_name_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:29:56.541644Z",
     "start_time": "2024-04-13T12:29:51.483722Z"
    }
   },
   "id": "33ec270c06295aab",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fe7fb2372b9d6bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data transformation chain.\n",
    "transformation_chain = T.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256 and then we take center crop.\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop((192, 192)),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:29:34.583904Z",
     "start_time": "2024-04-13T12:29:34.579671Z"
    }
   },
   "id": "e0e4e9a2cfba4de4",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfd47b2877bba76b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from image_similarity.data_preprocesing import create_dataset, get_idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:22:55.906047Z",
     "start_time": "2024-04-13T12:22:55.388104Z"
    }
   },
   "id": "3ba063b6ae49d8cc",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train, test = create_dataset(\"../data/my_dataset\", max_num=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T09:33:15.034127Z",
     "start_time": "2024-04-13T09:33:15.030731Z"
    }
   },
   "id": "71a77987c652a579",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train, test = create_dataset(\"../data/train\", max_num=10000, train_test_percentage=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T09:33:26.636377Z",
     "start_time": "2024-04-13T09:33:15.035127Z"
    }
   },
   "id": "c3e3e645e66898",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train, test = create_dataset(\"train_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:23:13.240024Z",
     "start_time": "2024-04-13T12:22:58.616254Z"
    }
   },
   "id": "a788f646c512f98",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_embeddings.csv\")\n",
    "# df['img_path'] = df['img_path'].apply(lambda x: x.replace(\"..\", \"E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\"))\n",
    "df.head()\n",
    "df.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:20:21.210966Z",
     "start_time": "2024-04-13T13:20:18.712373Z"
    }
   },
   "id": "5b7e2d74d9fe1c5c",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.to_csv(\"train_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:20:35.924674Z",
     "start_time": "2024-04-13T13:20:22.267805Z"
    }
   },
   "id": "8592892c45271ade",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "20345"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"object_id\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:57:58.399588Z",
     "start_time": "2024-04-13T13:57:58.349424Z"
    }
   },
   "id": "cea7e1821458b13a",
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "16430"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"object_id\"].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:58:07.880508Z",
     "start_time": "2024-04-13T13:58:07.866236Z"
    }
   },
   "id": "c713ae72067de721",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['image_file_path', 'image', 'labels'],\n    num_rows: 16276\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:19:21.012443Z",
     "start_time": "2024-04-13T12:19:21.006965Z"
    }
   },
   "id": "88bf211e66926e0f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = pd.Series([1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T15:52:49.767451Z",
     "start_time": "2024-04-13T15:52:49.756432Z"
    }
   },
   "id": "e2faeaa57294a3cc",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_samples = len(train)\n",
    "seed = 42\n",
    "batch_size = 64\n",
    "candidate_subset = train.shuffle(seed=seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:25:20.522800Z",
     "start_time": "2024-04-13T12:25:20.515805Z"
    }
   },
   "id": "99d5adb1fbea0361",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_embeddings(batch):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    return {\"embeddings\": batch[\"image\"]}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:25:21.238081Z",
     "start_time": "2024-04-13T12:25:21.235347Z"
    }
   },
   "id": "9518ee06895a56b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/16276 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77b8acb2b7f74c649b7dc7c37f78af64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "candidate_subset_emb = candidate_subset.map(extract_embeddings, batched=True, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:27:52.139494Z",
     "start_time": "2024-04-13T12:25:34.435440Z"
    }
   },
   "id": "47492507eedb2ef0",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_candidate_embeddings = torch.from_numpy(np.array(candidate_subset_emb[\"embeddings\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:28:01.374308Z",
     "start_time": "2024-04-13T12:27:55.081574Z"
    }
   },
   "id": "e6606246596bca9c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/16276 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49d1ef5423dc4e9e9068bcad99ea6017"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "candidate_ids = get_idx(candidate_subset_emb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:28:29.441162Z",
     "start_time": "2024-04-13T12:28:03.855851Z"
    }
   },
   "id": "b11e747be4148c8f",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f578d3ad5c41f3a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOV8 model\n",
    "\n",
    "yolo_model = YOLO(r\"E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\\classification\\runs\\classify\\train11\\weights\\best.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:33:20.815868Z",
     "start_time": "2024-04-13T12:33:19.459460Z"
    }
   },
   "id": "7a02d54ca6492949",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_img = test['image']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:28:30.945075Z",
     "start_time": "2024-04-13T12:28:30.942340Z"
    }
   },
   "id": "f2070c77f4e6b7a1",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import cv2\n",
    "\n",
    "def predict_top_k(img: Union[int, str], k=10):\n",
    "    sample = Image.open(img) # test_img[img] if isinstance(img, int) else \n",
    "    pred = yolo_model.predict(img)\n",
    "    group = [d.index(max(d)) for d in [c.cpu().numpy().tolist() for c in [r.probs.data for r in pred]]][0]\n",
    "    group_name = pred[0].names[group]\n",
    "\n",
    "    sample = [transformation_chain(sample)]\n",
    "    with torch.no_grad():\n",
    "        embeddings_model = get_embeddings(processor=processor_1,\n",
    "                                      model=model_1,\n",
    "                                      image=sample)\n",
    "        embeddings = torch.from_numpy(np.insert(np.array(embeddings_model.data.cpu()).squeeze(0), 0, group))\n",
    "        # embeddings_2 = get_embeddings(processor=processor_2,\n",
    "        #                               model=model_2,\n",
    "        #                               image=sample)\n",
    "\n",
    "    # emb = torch.hstack((embeddings_1, embeddings_2))\n",
    "    sim_idx, sim_labels, sim_paths, sim_scores = fetch_similar(query_embeddings=embeddings, all_embeddings=all_candidate_embeddings, idx=candidate_ids, top_k=k)\n",
    "    print(*zip(sim_labels, sim_scores), sep=\"\\n\")\n",
    "    orig_path = cv2.imread(test['image_file_path'][img] if isinstance(img, int) else img)\n",
    "\n",
    "    images = [cv2.imread(x) for x in sim_paths]\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        images[i] = cv2.resize(images[i], (255, 255))\n",
    "\n",
    "    # concatenate image Vertically \n",
    "    vert = np.concatenate(images, axis=0)\n",
    "\n",
    "    cv2.imwrite(\"OrigImg.jpg\", orig_path)\n",
    "    cv2.imwrite('TopK.jpg', vert)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:49:58.998502Z",
     "start_time": "2024-04-13T12:49:58.992299Z"
    }
   },
   "id": "1828c246962be24e",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\\data\\plastinka.jpg: 256x256  1.00,  0.00,  0.00,  0.00,  0.00, 78.7ms\n",
      "Speed: 9.0ms preprocess, 78.7ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 256)\n",
      "('22796825', 0.7540040016174316)\n",
      "('25585108', 0.7539374828338623)\n",
      "('10320706', 0.7452168464660645)\n",
      "('11285091', 0.7439494729042053)\n",
      "('8720483', 0.7431620955467224)\n",
      "('22924337', 0.7420465350151062)\n",
      "('9776990', 0.7379002571105957)\n",
      "('44778134', 0.736646831035614)\n",
      "('17101370', 0.7337498068809509)\n",
      "('43800883', 0.7335513830184937)\n"
     ]
    }
   ],
   "source": [
    "predict_top_k(os.path.join(\"E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\\data\", \"plastinka.jpg\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:50:35.436831Z",
     "start_time": "2024-04-13T12:50:34.676979Z"
    }
   },
   "id": "95725c250c667edd",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\\data\\4701371.jpg: 256x256  1.00,  0.00,  0.00,  0.00,  0.00, 85.7ms\n",
      "Speed: 4.0ms preprocess, 85.7ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 256)\n",
      "('4701371', 0.9402170181274414)\n",
      "('6339689', 0.8821889758110046)\n",
      "('5029921', 0.8777577877044678)\n",
      "('4708600', 0.8774980306625366)\n",
      "('4702385', 0.8773635029792786)\n",
      "('4707704', 0.8772224187850952)\n",
      "('9872355', 0.875396728515625)\n",
      "('10713733', 0.8740614652633667)\n",
      "('9871462', 0.8736203908920288)\n",
      "('4704535', 0.8733720183372498)\n"
     ]
    }
   ],
   "source": [
    "predict_top_k(os.path.join(\"E:\\MachineLearningProjects\\ml-practices\\src\\haccaton\\data\", \"4701371.jpg\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T12:51:03.773090Z",
     "start_time": "2024-04-13T12:51:03.211168Z"
    }
   },
   "id": "512ed4026dcc10e1",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ideas how to upgrade\n",
    "\n",
    "predict_top_k by k-neighbors \n",
    "\n",
    "Structural Similarity Index (SSIM) - ! with gray scale\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "Calulate mean square error\n",
    "\n",
    "def MeanSquareError(image1, image2):\n",
    "    imageError= np.sum((image1.astype(\"float32\")-image2.astype(\"float32\"))**2)\n",
    "    imageError = imageError/float(image1.shape[0]*image2.shape[1])\n",
    "    return imageError\n",
    "\n",
    "def ImageComparision (image1, image2):\n",
    "    mean = MeanSquareError(image1, image2)\n",
    "    Ssim= ssim(image1, image2)\n",
    "    print(f\"Mean Square Error is {mean}\\n Structural Similarity Index{Ssim}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca9b022ec15118cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
